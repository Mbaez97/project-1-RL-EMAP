{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1006611f",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "\n",
    "This project implements Reinforcement Learning (RL) for the Recycling Robot problem (Example 3.3 in the textbook).\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Use the Temporal Difference (TD) algorithm so the robot learns to maximize the total reward over time.\n",
    "\n",
    "Key choices and requirements:\n",
    "\n",
    "- Select values for the probabilities alpha (α) and beta (β), and for the rewards r_search and r_wait, ensuring r_search > r_wait.\n",
    "- Use TD learning to update value estimates and derive a policy that maximizes accumulated reward.\n",
    "- Define a learning period (epoch) of, for example, 1000 steps. After each epoch, save the total reward accumulated during that epoch to a file named `rewards.txt`.\n",
    "- Train for multiple epochs and plot the accumulated total reward over epochs (Matplotlib or Seaborn). Optionally repeat training runs and average the curves to smooth results.\n",
    "- Visualize the learned (optimal) policy as a heatmap (Seaborn), where each cell reflects the preferred action(s) for each robot state.\n",
    "\n",
    "See `README.md` for the short project title and overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db39e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224b9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(Enum):\n",
    "    \"\"\"States for the recycling robot.\"\"\"\n",
    "\n",
    "    HIGH = 0  # High battery\n",
    "    LOW = 1  # Low battery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc062098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    \"\"\"Actions the robot can take.\"\"\"\n",
    "\n",
    "    SEARCH = 0\n",
    "    WAIT = 1\n",
    "    RECHARGE = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901d5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecyclingRobot:\n",
    "    \"\"\"\n",
    "    The Recycling Robot Environment.\n",
    "\n",
    "    This class simulates the environment based on the problem description for a\n",
    "    recycling robot, which is a common example in reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha_prob=0.9, beta_prob=0.2, r_search=5.0, r_wait=1.0, r_rescue=-10.0\n",
    "    ):\n",
    "        \"\"\"Initializes the environment parameters.\"\"\"\n",
    "        self.states = list(State)\n",
    "        self.actions = list(Action)\n",
    "\n",
    "        self.alpha_prob = alpha_prob\n",
    "        self.beta_prob = beta_prob\n",
    "        self.r_search = r_search\n",
    "        self.r_wait = r_wait\n",
    "        self.r_rescue = r_rescue\n",
    "\n",
    "    def step(self, state: State, action: Action):\n",
    "        \"\"\"Simulates one step in the environment.\"\"\"\n",
    "        if state == State.HIGH:\n",
    "            if action == Action.SEARCH:\n",
    "                # With probability alpha_prob, battery stays high\n",
    "                if random.random() < self.alpha_prob:\n",
    "                    return State.HIGH, self.r_search\n",
    "                # Otherwise, it drains to low\n",
    "                return State.LOW, self.r_search\n",
    "            elif action == Action.WAIT:\n",
    "                # Stays at high, gets wait reward\n",
    "                return State.HIGH, self.r_wait\n",
    "            # RECHARGE is not a valid action at HIGH, but we handle it gracefully\n",
    "            return state, 0\n",
    "\n",
    "        elif state == State.LOW:\n",
    "            if action == Action.SEARCH:\n",
    "                # With probability beta_prob, it finds a can and stays low\n",
    "                if random.random() < self.beta_prob:\n",
    "                    return State.LOW, self.r_search\n",
    "                # Otherwise, battery dies, needs rescue (transitions to HIGH after rescue)\n",
    "                return State.HIGH, self.r_rescue\n",
    "            elif action == Action.WAIT:\n",
    "                # Stays at low, gets wait reward\n",
    "                return State.LOW, self.r_wait\n",
    "            elif action == Action.RECHARGE:\n",
    "                # Recharges to high\n",
    "                return State.HIGH, 0.0\n",
    "\n",
    "        # Should not be reached\n",
    "        return state, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e56b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_policy_evaluation(\n",
    "    env: RecyclingRobot, policy: dict, gamma: float, alpha: float, n_steps: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs TD(0) policy evaluation for a continuous task.\n",
    "    \"\"\"\n",
    "    # 1. Initialize V(s) arbitrarily (e.g., to 0)\n",
    "    V = {s: 0.0 for s in env.states}\n",
    "\n",
    "    # Start in a random state\n",
    "    current_state = random.choice(env.states)\n",
    "\n",
    "    print(f\"Running TD(0) for {n_steps} steps...\")\n",
    "    # Loop for a large number of steps\n",
    "    for i in range(n_steps):\n",
    "        # 2. Get action from the fixed policy\n",
    "        action = policy[current_state]\n",
    "\n",
    "        # 3. Take action, observe reward and next state\n",
    "        next_state, reward = env.step(current_state, action)\n",
    "\n",
    "        # 4. Update the value function using the TD update rule\n",
    "        # V(s) <- V(s) + alpha * [R + gamma * V(s') - V(s)]\n",
    "        td_target = reward + gamma * V[next_state]\n",
    "        td_error = td_target - V[current_state]\n",
    "        V[current_state] += alpha * td_error\n",
    "\n",
    "        # Move to the next state for the next iteration\n",
    "        current_state = next_state\n",
    "\n",
    "        if (i + 1) % (n_steps // 10) == 0:\n",
    "            print(\n",
    "                f\"  Step {i+1}/{n_steps} | V(HIGH)={V[State.HIGH]:.2f}, V(LOW)={V[State.LOW]:.2f}\"\n",
    "            )\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_policy_evaluation_run(env: RecyclingRobot, policy: dict, gamma: float, alpha: float, n_steps: int, V=None):\n",
    "    \"\"\"Run TD(0) for n_steps using an existing V (if provided) and return updated V and total reward.\"\"\"\n",
    "    if V is None:\n",
    "        V = {s: 0.0 for s in env.states}\n",
    "\n",
    "    current_state = random.choice(env.states)\n",
    "    total_reward = 0.0\n",
    "    for i in range(n_steps):\n",
    "        action = policy[current_state]\n",
    "        next_state, reward = env.step(current_state, action)\n",
    "        total_reward += reward\n",
    "        td_target = reward + gamma * V[next_state]\n",
    "        td_error = td_target - V[current_state]\n",
    "        V[current_state] += alpha * td_error\n",
    "        current_state = next_state\n",
    "    return V, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce74e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement_from_V(env: RecyclingRobot, V: dict, gamma: float):\n",
    "    \"\"\"Return a greedy policy w.r.t. value function V using the known environment dynamics.\"\"\"\n",
    "    policy = {}\n",
    "    for s in env.states:\n",
    "        q_vals = {}\n",
    "        for a in env.actions:\n",
    "            # compute expected immediate reward + gamma * expected V(next_state)\n",
    "            if s == State.HIGH:\n",
    "                if a == Action.SEARCH:\n",
    "                    alpha = env.alpha_prob\n",
    "                    q = env.r_search + gamma * (alpha * V[State.HIGH] + (1 - alpha) * V[State.LOW])\n",
    "                elif a == Action.WAIT:\n",
    "                    q = env.r_wait + gamma * V[State.HIGH]\n",
    "                elif a == Action.RECHARGE:\n",
    "                    # recharge at HIGH does nothing (treated as 0 reward and stays HIGH)\n",
    "                    q = 0.0 + gamma * V[State.HIGH]\n",
    "            else:  # LOW\n",
    "                if a == Action.SEARCH:\n",
    "                    beta = env.beta_prob\n",
    "                    q = env.r_search + gamma * (beta * V[State.LOW] + (1 - beta) * V[State.HIGH])\n",
    "                elif a == Action.WAIT:\n",
    "                    q = env.r_wait + gamma * V[State.LOW]\n",
    "                elif a == Action.RECHARGE:\n",
    "                    q = 0.0 + gamma * V[State.HIGH]\n",
    "            q_vals[a] = q\n",
    "        # pick a deterministic best action (first if tie)\n",
    "        best_action = max(q_vals, key=q_vals.get)\n",
    "        policy[s] = best_action\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd8c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def train_policy_iteration(env: RecyclingRobot, init_policy: dict, gamma: float, alpha: float, epoch_steps: int, n_epochs: int, rewards_file='rewards.txt'):\n",
    "    \"\"\"Perform simple policy-iteration-like training using TD(0) evaluation per epoch and greedy improvement.\"\"\"\n",
    "    # ensure rewards file is reset\n",
    "    open(rewards_file, 'w').close()\n",
    "    V = None\n",
    "    policy = dict(init_policy)\n",
    "    rewards = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        V, total_reward = td_policy_evaluation_run(env, policy, gamma, alpha, epoch_steps, V=V)\n",
    "        rewards.append(total_reward)\n",
    "        # append to file\n",
    "        with open(rewards_file, 'a') as f:\n",
    "            f.write(str(total_reward) + '\\n')\n",
    "        # policy improvement\n",
    "        policy = policy_improvement_from_V(env, V, gamma)\n",
    "        if epoch % max(1, n_epochs // 10) == 0:\n",
    "            print(f'Completed epoch {epoch}/{n_epochs} | total_reward={total_reward:.2f}')\n",
    "    return V, policy, rewards\n",
    "\n",
    "def plot_rewards(rewards, ax=None):\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        ax = plt.gca()\n",
    "    ax.plot(range(1, len(rewards)+1), rewards, marker='o', linewidth=1)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Total reward (per epoch)')\n",
    "    ax.set_title('Total reward per epoch')\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def plot_policy_heatmap(policy, env: RecyclingRobot, ax=None):\n",
    "    # rows: states in order [HIGH, LOW], cols: actions in order [SEARCH, WAIT, RECHARGE]\n",
    "    actions_order = [Action.SEARCH, Action.WAIT, Action.RECHARGE]\n",
    "    states_order = [State.HIGH, State.LOW]\n",
    "    matrix = [[1 if policy[s] == a else 0 for a in actions_order] for s in states_order]\n",
    "    labels = [[a.name if matrix[i][j] == 1 else '' for j,a in enumerate(actions_order)] for i in range(len(states_order))]\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(6,3))\n",
    "        ax = plt.gca()\n",
    "    sns.heatmap(matrix, annot=labels, fmt='', cmap='Blues', cbar=False, xticklabels=[a.name for a in actions_order], yticklabels=[s.name for s in states_order], ax=ax)\n",
    "    ax.set_title('Preferred action (1 = preferred)')\n",
    "    plt.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating policy: Always Search\n",
      "Running TD(0) for 50000 steps...\n",
      "  Step 5000/50000 | V(HIGH)=38.10, V(LOW)=25.08\n",
      "  Step 10000/50000 | V(HIGH)=35.46, V(LOW)=23.51\n",
      "  Step 15000/50000 | V(HIGH)=39.01, V(LOW)=24.42\n",
      "  Step 20000/50000 | V(HIGH)=39.79, V(LOW)=23.34\n",
      "  Step 25000/50000 | V(HIGH)=41.98, V(LOW)=23.40\n",
      "  Step 30000/50000 | V(HIGH)=37.59, V(LOW)=24.37\n",
      "  Step 35000/50000 | V(HIGH)=38.34, V(LOW)=24.96\n",
      "  Step 40000/50000 | V(HIGH)=34.03, V(LOW)=21.23\n",
      "  Step 45000/50000 | V(HIGH)=35.45, V(LOW)=22.25\n",
      "  Step 50000/50000 | V(HIGH)=34.11, V(LOW)=22.42\n",
      "--- Results for 'Always Search' Policy ---\n",
      "  V(HIGH) = 34.11\n",
      "  V(LOW) = 22.42\n",
      "==================================================\n",
      "Evaluating policy: Search when HIGH, Recharge when LOW\n",
      "Running TD(0) for 50000 steps...\n",
      "  Step 5000/50000 | V(HIGH)=44.77, V(LOW)=40.44\n",
      "  Step 10000/50000 | V(HIGH)=45.83, V(LOW)=40.96\n",
      "  Step 15000/50000 | V(HIGH)=45.24, V(LOW)=40.25\n",
      "  Step 20000/50000 | V(HIGH)=44.96, V(LOW)=40.55\n",
      "  Step 25000/50000 | V(HIGH)=45.67, V(LOW)=40.11\n",
      "  Step 30000/50000 | V(HIGH)=45.13, V(LOW)=40.55\n",
      "  Step 35000/50000 | V(HIGH)=44.49, V(LOW)=39.74\n",
      "  Step 40000/50000 | V(HIGH)=45.94, V(LOW)=40.99\n",
      "  Step 45000/50000 | V(HIGH)=45.25, V(LOW)=41.00\n",
      "  Step 50000/50000 | V(HIGH)=45.80, V(LOW)=40.78\n",
      "--- Results for 'Conservative' Policy ---\n",
      "  V(HIGH) = 45.80\n",
      "  V(LOW) = 40.78\n"
     ]
    }
   ],
   "source": [
    "# Train with policy-iteration-like loop and plot rewards + policy heatmap\n",
    "env = RecyclingRobot(alpha_prob=0.9, beta_prob=0.2, r_search=5.0, r_wait=1.0, r_rescue=-10.0)\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "epoch_steps = 1000\n",
    "n_epochs = 50\n",
    "\n",
    "# initial policy: search when HIGH, recharge when LOW (conservative starting point)\n",
    "init_policy = {State.HIGH: Action.SEARCH, State.LOW: Action.RECHARGE}\n",
    "V_final, learned_policy, rewards = train_policy_iteration(env, init_policy, discount_factor, learning_rate, epoch_steps, n_epochs, rewards_file='rewards.txt')\n",
    "\n",
    "print('\\nFinal value estimates:')\n",
    "for s, v in V_final.items():\n",
    "    print(f'  V({s.name}) = {v:.2f}')\n",
    "\n",
    "print('\\nLearned policy:')\n",
    "for s, a in learned_policy.items():\n",
    "    print(f'  pi({s.name}) = {a.name}')\n",
    "\n",
    "# plot rewards\n",
    "plot_rewards(rewards)\n",
    "plt.show()\n",
    "\n",
    "# plot policy heatmap\n",
    "plot_policy_heatmap(learned_policy, env)\n",
    "plt.show()\n",
    "\n",
    "print('\\nSaved epoch rewards to rewards.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
