{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1006611f",
   "metadata": {},
   "source": [
    "# Problem Description\n",
    "\n",
    "This project implements Reinforcement Learning (RL) for the Recycling Robot problem (Example 3.3 in the textbook).\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Use the Temporal Difference (TD) algorithm so the robot learns to maximize the total reward over time.\n",
    "\n",
    "Key choices and requirements:\n",
    "\n",
    "- Select values for the probabilities alpha (α) and beta (β), and for the rewards r_search and r_wait, ensuring r_search > r_wait.\n",
    "- Use TD learning to update value estimates and derive a policy that maximizes accumulated reward.\n",
    "- Define a learning period (epoch) of, for example, 1000 steps. After each epoch, save the total reward accumulated during that epoch to a file named `rewards.txt`.\n",
    "- Train for multiple epochs and plot the accumulated total reward over epochs (Matplotlib or Seaborn). Optionally repeat training runs and average the curves to smooth results.\n",
    "- Visualize the learned (optimal) policy as a heatmap (Seaborn), where each cell reflects the preferred action(s) for each robot state.\n",
    "\n",
    "See `README.md` for the short project title and overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c122cef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python /home/marcelo_baez/phd/project-1-RL-EMAP/recycling_robot.py\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
